{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ff614cf",
   "metadata": {},
   "source": [
    "## Connect to ArcGIS Enterprise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac502d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from arcgis import GIS, geoanalytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151923b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the ArcGIS Enterprise deployment\n",
    "gis = GIS(profile=\"dev_summit_demo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee5cc39",
   "metadata": {},
   "source": [
    "## Check if GeoAnalytics is supported"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a14a96",
   "metadata": {},
   "source": [
    "**Note:** The tools available in the `arcgis.geoanalytics` module require that the ArcGIS Enterprise deployment be configured with at least one ArcGIS GeoAnalytics server."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2be87e",
   "metadata": {},
   "source": [
    "`geoanalytics.is_supported()` returns `True` if the GIS supports GeoAnalytics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f070b73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure GeoAnalytics is supported\n",
    "geoanalytics.is_supported()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ec21b7",
   "metadata": {},
   "source": [
    "## Big Data File Shares"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958289ee",
   "metadata": {},
   "source": [
    "A big data file share is an item created in your portal that references a location available to your ArcGIS GeoAnalytics Server. You can use the big data file share location as an input and output to feature data (points, polylines, polygons, and tabular data) of GeoAnalytics tools.\n",
    "\n",
    "<br/>\n",
    "\n",
    "There are several benefits to using a big data file share:\n",
    "  - A big data file share accesses the data when the analysis is run, so you can continue to add data to an existing dataset in your big data file share without having to reregister or publish your data.\n",
    " \n",
    " \n",
    "  - You can also [modify the manifest](https://enterprise.arcgis.com/en/geoanalytics/10.9.1/perform-analysis/what-is-a-big-data-file-share.htm#ESRI_SECTION1_B5813F52FD444398AF4F122F38D9FD46) to remove, add, or update datasets in the big data file share.\n",
    " \n",
    " \n",
    "  - Big data file shares also allow you to partition your datasets while still treating multiple partitions as a single dataset.\n",
    "  \n",
    "  \n",
    "  - Using big data file shares for output data allows you to store your results in formats that you may use for other workflows, such as a parquet file for further analysis or storage.\n",
    "  \n",
    "<br/>\n",
    "  \n",
    "**Note:** Big data file shares are only accessed when you run GeoAnalytics Tools. This means that you can only browse and add big data files to your analysis; you cannot visualize the data on a map.\n",
    "\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b3380a",
   "metadata": {},
   "source": [
    "Big data file shares can reference the following input data sources:\n",
    "  - **File share** - A directory of datasets on a local disk or network share.\n",
    "  - **Apache Hadoop Distributed File System (HDFS)** - An HDFS directory of datasets.\n",
    "  - **Apache Hive** - Hive metastore databases.\n",
    "  - **Cloud store** - An Amazon Simple Storage Service (S3) bucket, Microsoft Azure Blob container, or Microsoft Azure Data Lake (Server Manager only) store containing a directory of datasets.\n",
    "  \n",
    "<br/>\n",
    "\n",
    "The following file types are supported as datasets for input and output in big data file shares:\n",
    "  - **Delimited files** - (such as .csv, .tsv, and .txt)\n",
    "  - **Shapefiles** - (.shp)\n",
    "  - **Parquet files** - (.gz.parquet)\n",
    "  - **ORC files** - (orc.crc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8b2d8a",
   "metadata": {},
   "source": [
    "Example of a big data file share that contains three datasets: Earthquakes, Hurricanes, and GlobalOceans."
   ]
  },
  {
   "cell_type": "raw",
   "id": "b1043b96",
   "metadata": {},
   "source": [
    "|---FileShareFolder                 < -- The top-level folder is what is registered as a big data file share\n",
    "   |---Earthquakes                  < -- A dataset \"Earthquakes\", composed of 4 csvs with the same schema\n",
    "      |---1960\n",
    "         |---01_1960.csv\n",
    "         |---02_1960.csv\n",
    "      |---1961\n",
    "         |---01_1961.csv\n",
    "         |---02_1961.csv\n",
    "   |---Hurricanes                   < -- The dataset \"Hurricanes\", composed of 3 shapefiles with the same schema\n",
    "      |---atlantic_hur.shp\n",
    "      |---pacific_hur.shp\n",
    "      |---otherhurricanes.shp\n",
    "   |---GlobalOceans                 < -- The dataset \"GlobalOceans\", composed of a single shapefile\n",
    "      |---oceans.shp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09638bd4",
   "metadata": {},
   "source": [
    "## Access GeoAnalytics data stores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3978042e",
   "metadata": {},
   "source": [
    "`geoanalytics.get_datastores()` returns an instance of the [DatastoreManager](https://developers.arcgis.com/python/api-reference/arcgis.gis.toc.html?highlight=add_bigdata#datastoremanager) helper class, which is used to manage data stores within the ArcGIS Enterprise deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e70f318",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the GeoAnalytics data stores\n",
    "gax_datastores = geoanalytics.get_datastores()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2025b69a",
   "metadata": {},
   "source": [
    "## Register data on AWS S3 as a Cloud Store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68dd7ebe",
   "metadata": {},
   "source": [
    "When you register a cloud store, you must include an Azure container name, an Amazon S3 bucket name, or an Azure Data Lake Store account name. It is recommended that you additionally specify a folder within the container or bucket. The specified folder is composed of subfolders, and each represents an individual dataset. Each dataset is composed of all the contents of the subfolder.\n",
    "\n",
    "**Note:** To register a cloud store as a big data file share, you must first add the cloud store as a registered data store.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973f5b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a unique name for the Cloud Store\n",
    "import uuid\n",
    "unique_cloud_store_name = \"demo_cloud_store_{0}\".format(str(uuid.uuid4())[:6])\n",
    "unique_cloud_store_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee50975",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the AWS S3 accessKeyId and secretAccessKey from local environment variables \n",
    "import os\n",
    "aws_access_key_id = os.getenv(\"AWS_ACCESS_KEY_ID\")\n",
    "aws_secret_access_key = os.getenv(\"AWS_SECRET_ACCESS_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b3077d",
   "metadata": {},
   "source": [
    "Connection information for the Cloud Store is stored within the [connectionString](https://developers.arcgis.com/rest/enterprise-administration/server/dataitem.htm#GUID-C2B4950B-5CA6-4732-8985-9AB360EA3633)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d10a97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the connection string for the Cloud Store\n",
    "cloud_store_connection_string_json = {\"accessKeyId\": aws_access_key_id,\n",
    "                                      \"secretAccessKey\": aws_secret_access_key,\n",
    "                                      \"regionEndpointUrl\": \"s3.us-west-1.amazonaws.com\",\n",
    "                                      \"region\": \"us-west-1\",\n",
    "                                      \"defaultEndpointsProtocol\": \"https\",\n",
    "                                      \"credentialType\": \"accesskey\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1ff0af",
   "metadata": {},
   "source": [
    "The [add_cloudstore](https://developers.arcgis.com/python/api-reference/arcgis.gis.toc.html?#arcgis.gis.DatastoreManager.add_cloudstore) method adds a Cloud Store data [Item](https://developers.arcgis.com/python/api-reference/arcgis.gis.toc.html?#arcgis.gis.Item)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628e9931",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the Cloud Store with the GeoAnalytics server\n",
    "registered_cloud_store = gax_datastores.add_cloudstore(name=unique_cloud_store_name,\n",
    "                                                       conn_str=cloud_store_connection_string_json,\n",
    "                                                       object_store=\"esri-ga-test-bdfs/dev_summit_bdfs\",\n",
    "                                                       provider=\"amazon\"\n",
    "                                                      )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b435ff3",
   "metadata": {},
   "source": [
    "The [validate](https://developers.arcgis.com/python/api-reference/arcgis.gis.toc.html?highlight=datastore#arcgis.gis.Datastore.validate) method validates all items in the datastore.  Returns `True` if validation was successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e733e415",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate the Cloud Store item\n",
    "registered_cloud_store.validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a17b7e6",
   "metadata": {},
   "source": [
    "### Register the AWS S3 Cloud Store as a Big Data File Share"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b894b38",
   "metadata": {},
   "source": [
    "Big Data File Share data items are file shares, HDFS, Hive, or cloud data stores that contain input data for GeoAnalytics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700ef5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a unique name for the Big Data File Share\n",
    "unique_bdfs_name = \"demo_bdfs_{0}\".format(str(uuid.uuid4())[:6])\n",
    "unique_bdfs_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0cde067",
   "metadata": {},
   "source": [
    "The [add_bigdata](https://developers.arcgis.com/python/api-reference/arcgis.gis.toc.html?highlight=add_bigdata#arcgis.gis.DatastoreManager.add_bigdata) method registers a big data file share with the Datastore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72f1215",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the AWS S3 Cloud Store as a Big Data File Share\n",
    "registered_bdfs = gax_datastores.add_bigdata(name=unique_bdfs_name,\n",
    "                                             server_path=registered_cloud_store.path,\n",
    "                                             connection_type=\"dataStore\"\n",
    "                                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2924d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate the Big Data File Share\n",
    "registered_bdfs.validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47c8c66",
   "metadata": {},
   "source": [
    "## Big Data File Share manifests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37cf2e3",
   "metadata": {},
   "source": [
    "Big data file shares require a [manifest](https://enterprise.arcgis.com/en/geoanalytics/latest/perform-analysis/understanding-the-big-data-file-share-manifest.htm) to outline the schema of the data, as well as the fields that represent geometry and time in the dataset.\n",
    "\n",
    "The manifest is automatically generated when you register a big data file share, but you may need to make modifications if there are any changes to your data, or if the manifest generation was unable to determine all the information needed (for example, if the automatically generated manifest did not select the correct field for the geometry or time)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c8d41a",
   "metadata": {},
   "source": [
    "The [manifest](https://developers.arcgis.com/python/api-reference/arcgis.gis.toc.html?highlight=datastore#arcgis.gis.Datastore.manifest) property retrieves or sets the manifest resource for big data file shares, as a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb8a2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the Big Data File Share manifest\n",
    "manifest = registered_bdfs.manifest\n",
    "manifest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d5cdc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function that prints the field names for each dataset in a Big Data File Share\n",
    "def print_bdfs_item_dataset_fields(bdfs_name):\n",
    "    # Get the BDFS item\n",
    "    bdfs_item = gis.content.search(query=\"title:{0}\".format(bdfs_name),\n",
    "                                   item_type=\"big data file share\",\n",
    "                                   max_items=1)[0]\n",
    "    \n",
    "    # Print the field names for each dataset\n",
    "    for layer in bdfs_item.layers:\n",
    "        print(\"{0}:\".format(layer.properties.name))\n",
    "        for field in layer.properties.fields:\n",
    "            print(\"\\t - \" + field.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0281d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the field names for each dataset in the registered Big Data File Share \n",
    "print_bdfs_item_dataset_fields(unique_bdfs_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac58e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the first field name of the first dataset in the Big Data File Share\n",
    "manifest[\"datasets\"][0][\"schema\"][\"fields\"][0].update({'name': \"UPDATED_FIELD_NAME\"})\n",
    "\n",
    "# Update the Big Data File Share manifest\n",
    "registered_bdfs.manifest = manifest\n",
    "registered_bdfs.manifest[\"datasets\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657fd99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the field names for each dataset in the registered Big Data File Share \n",
    "print_bdfs_item_dataset_fields(unique_bdfs_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf463d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the first dataset from the Big Data File Share\n",
    "manifest = registered_bdfs.manifest\n",
    "del manifest[\"datasets\"][0]\n",
    "\n",
    "# Update the Big Data File Share manifest\n",
    "registered_bdfs.manifest = manifest\n",
    "registered_bdfs.manifest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717d7241",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the field names for each dataset in the registered Big Data File Share \n",
    "print_bdfs_item_dataset_fields(unique_bdfs_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86246301",
   "metadata": {},
   "source": [
    "The [regenerate](https://developers.arcgis.com/python/api-reference/arcgis.gis.toc.html?highlight=datastore#arcgis.gis.Datastore.regenerate) method is used to regenerate the manifest for a big data file share. Returns `True` if the manifest was regenerated successfully. For example you would regenerate the Big Data File Share manifest if you added new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0cab944",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regenerate the Big Data File Share manifest\n",
    "registered_bdfs.regenerate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19057d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate the Big Data File Share\n",
    "registered_bdfs.validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa50f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the Big Data File Share manifest\n",
    "registered_bdfs.manifest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997e47a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the field names for each dataset in the registered Big Data File Share \n",
    "print_bdfs_item_dataset_fields(unique_bdfs_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ffc6103",
   "metadata": {},
   "source": [
    "## Search for Big Data File Shares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81b3bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the registered Big Data File Share item\n",
    "bdfs_item = gis.content.search(query=\"title:{0}\".format(unique_bdfs_name),\n",
    "                               item_type=\"big data file share\",\n",
    "                               max_items=1)[0]\n",
    "bdfs_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8557711",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function that prints the name of each layer in a Big Data File Share\n",
    "def print_bdfs_item_layers(bdfs_item):\n",
    "    for index, item in enumerate(bdfs_item.layers):\n",
    "        print(\"{0}: {1}\".format(index, item.properties.name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b756dc0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the names of the layers in the registered Big Data File Share\n",
    "print_bdfs_item_layers(bdfs_item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1619c5",
   "metadata": {},
   "source": [
    "Items that have layers have a dynamic [layers](https://developers.arcgis.com/python/api-reference/arcgis.gis.toc.html?#item) property that is used to get the individual layers in the item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293202e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the UberSF data from the Big Data File Share\n",
    "ubersf_bdfs_layer = bdfs_item.layers[1]\n",
    "ubersf_bdfs_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406272e5",
   "metadata": {},
   "source": [
    "## Return GeoAnalytics tool job messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ccdfe9",
   "metadata": {},
   "source": [
    "[GPJob](https://developers.arcgis.com/python/api-reference/arcgis.geoprocessing.html?highlight=gpjob#gpjob) represents a single geoprocessing job. The GPJob class allows for the asynchronous operation of any geoprocessing task. To request a GPJob task, the tool must be called with `future=True` or else the operation will occur synchronously.\n",
    "\n",
    "`GPJob.done()` returns `True` if the call was successfully cancelled or finished running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133e941b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Create a function that prints the job messages while a GPJob task is running\n",
    "def print_job_messages(gpjob):\n",
    "    previous_message = None\n",
    "    while gpjob.done() is False:\n",
    "        if gpjob.messages:\n",
    "            current_message = gpjob.messages[-1][\"description\"]\n",
    "            if current_message != previous_message:\n",
    "                if \"messageCode\" in current_message:\n",
    "                    print(json.loads(current_message)[\"message\"])\n",
    "                previous_message = current_message"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e76cb3",
   "metadata": {},
   "source": [
    "## Use GeoAnalytics tools to analyze the UberSF trips data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2705fe1d",
   "metadata": {},
   "source": [
    "### Describe Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fae211b",
   "metadata": {},
   "source": [
    "The GeoAnalytics [Describe Dataset](https://developers.arcgis.com/rest/services-reference/enterprise/describe-dataset.htm) tool summarizes features into calculated field statistics, sample features, and extent boundaries.\n",
    "\n",
    "The sample layer allows you to efficiently test your workflow before running it on the full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f4ff6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the GeoAnalytics Describe Dataset tool\n",
    "from arcgis.geoanalytics.summarize_data import describe_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7b6243",
   "metadata": {},
   "source": [
    "### Describe Dataset: GPJob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc20fa9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a unique output name\n",
    "unique_output_name = \"dd_ubersf_gpjob_{0}\".format(str(uuid.uuid4())[:6])\n",
    "print(\"Output name: {0}\\n\".format(unique_output_name))\n",
    "\n",
    "# Run the Describe Dataset tool on the UberSF data with future=True\n",
    "dd_ubersf_gpjob = describe_dataset(input_layer=ubersf_bdfs_layer,\n",
    "                                   extent_output=True,\n",
    "                                   sample_size=1000,\n",
    "                                   output_name=unique_output_name,\n",
    "                                   future=True  # This returns a GPJob\n",
    "                                  )\n",
    "\n",
    "# Print the job messages while the tool is running\n",
    "print_job_messages(dd_ubersf_gpjob)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb60cd3",
   "metadata": {},
   "source": [
    "`GPJob.result()` will return the value returned by the call. If the call hasn’t yet completed then this method will wait."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831b1857",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the Describe Dataset GPJob result\n",
    "dd_ubersf_gpjob.result()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36fb67cc",
   "metadata": {},
   "source": [
    "### Describe Dataset: Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d2800d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a unique output name\n",
    "unique_output_name = \"dd_ubersf_result_{0}\".format(str(uuid.uuid4())[:6])\n",
    "print(\"Output name: {0}\\n\".format(unique_output_name))\n",
    "\n",
    "# Run the Describe Dataset tool on the UberSF data with future=False\n",
    "dd_ubersf_result = describe_dataset(input_layer=ubersf_bdfs_layer,\n",
    "                                    extent_output=True,\n",
    "                                    sample_size=1000,\n",
    "                                    output_name=unique_output_name,\n",
    "                                    future=False\n",
    "                                    )\n",
    "\n",
    "# Show the Describe Dataset result\n",
    "dd_ubersf_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f31ef3",
   "metadata": {},
   "source": [
    "### Visualize the Describe Dataset result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8659acbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a map of San Francisco\n",
    "map_one = gis.map(\"San Francisco\")\n",
    "map_one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9cd1c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the Describe Dataset extent layer and the sample layer to the map\n",
    "map_one.add_layer(dd_ubersf_result.layers[0], {\"opacity\": 0.5})\n",
    "map_one.add_layer(dd_ubersf_result.layers[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f457c9df",
   "metadata": {},
   "source": [
    "### View the Describe Dataset summary statistics table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f086858",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48301435",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Spatially Enabled DataFrame object from the Describe Dataset summary statistics table\n",
    "sdf = pd.DataFrame.spatial.from_layer(dd_ubersf_result.tables[0])\n",
    "sdf[[\"FIELD_NAME\", \"COUNT\", \"COUNT_NON_EMPTY\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d10d874",
   "metadata": {},
   "source": [
    "### Reconstruct Tracks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d891c2",
   "metadata": {},
   "source": [
    "The GeoAnalytics [Reconstruct Tracks](https://developers.arcgis.com/rest/services-reference/enterprise/reconstruct-tracks.htm) tool creates line or polygon tracks from time-enabled input data.  The features must represent an instant in time.\n",
    "\n",
    "The tool first determines which features belong to a track using an identifier. Using the time at each location, the tracks are ordered sequentially and transformed into a line or polygon representing the path of movement over time. Optionally, the input can be buffered by a field, which creates a polygon at each location.\n",
    "\n",
    "<br/>\n",
    "\n",
    "<img src=\"https://pro.arcgis.com/en/pro-app/latest/tool-reference/big-data-analytics/GUID-85767AB0-D12E-4923-9C22-FE2A758DF149-web.png\" width=\"500\" align=\"left\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533b4139",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the GeoAnalytics Reconstruct Tracks tool\n",
    "from arcgis.geoanalytics.summarize_data import reconstruct_tracks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6196e378",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a unique output name\n",
    "unique_output_name = \"rt_ubersf_{0}\".format(str(uuid.uuid4())[:6])\n",
    "print(\"Output name: {0}\\n\".format(unique_output_name))\n",
    "\n",
    "# Run the Reconstruct Tracks tool on the UberSF data\n",
    "rf_ubersf_output = reconstruct_tracks(input_layer=ubersf_bdfs_layer,\n",
    "                                      track_fields=\"id\",\n",
    "                                      output_name=unique_output_name,\n",
    "                                      future=True  # This returns a GPJob\n",
    "                                      )\n",
    "\n",
    "# Print the job messages while the tool is running\n",
    "print_job_messages(rf_ubersf_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5379e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a map of South San Francisco\n",
    "map_two = gis.map(\"South San Francisco\")\n",
    "map_two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333c3454",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the Reconstruct Tracks layer to the map\n",
    "map_two.add_layer(rf_ubersf_output.result().layers[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e50cd91",
   "metadata": {},
   "source": [
    "### Summarize Within"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203e48dd",
   "metadata": {},
   "source": [
    "The GeoAnalytics [Summarize Within](https://developers.arcgis.com/rest/services-reference/enterprise/summarize-within.htm) tool overlays a polygon layer with another layer to summarize the number of points, length of the lines, or area of the polygons within each polygon and calculates attribute field statistics about those features within the polygons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b722ec85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the GeoAnalytics Summarize Within tool\n",
    "from arcgis.geoanalytics.summarize_data import summarize_within"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4163931d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a unique output name\n",
    "unique_output_name = \"sw_ubersf_rt_{0}\".format(str(uuid.uuid4())[:6])\n",
    "print(\"Output name: {0}\\n\".format(unique_output_name))\n",
    "\n",
    "\n",
    "# Run the Summarize Within tool on the UberSF data reconstructed tracks\n",
    "sw_ubersf_output = summarize_within(summarized_layer=rf_ubersf_output.result().layers[0],\n",
    "                                    bin_type=\"Hexagon\",\n",
    "                                    bin_size=0.5,\n",
    "                                    bin_size_unit=\"Miles\",\n",
    "                                    output_name=unique_output_name,\n",
    "                                    future=True  # This returns a GPJob\n",
    "                                    )\n",
    "\n",
    "# Print the job messages while the tool is running\n",
    "print_job_messages(sw_ubersf_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b7e29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a map of San Francisco\n",
    "map_three = gis.map(\"San Francisco\")\n",
    "map_three"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ffb467e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the Summarize Within layer to the map\n",
    "map_three.add_layer(sw_ubersf_output.result().layers[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
